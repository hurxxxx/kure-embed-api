# KURE v1 성능 측정 리포트

## 테스트 환경
- **GPU**: NVIDIA RTX 3090 (24GB VRAM)
- **모델**: KURE v1 (nlpai-lab/KURE-v1)
- **비교 대상**: OpenAI text-embedding-3-small/large
- **테스트 날짜**: 2025년 1월
- **환경**: GPU 서버 (Ubuntu, CUDA 12.1)

## 측정 조건 및 제한사항
- **KURE v1**: 로컬 GPU 서버에서 직접 처리 (네트워크 지연 없음)
- **OpenAI**: API 호출을 통한 처리 (네트워크 전송 시간 포함)
- **네트워크 환경**: 일반적인 인터넷 연결 환경
- **주의사항**: 처리 시간 비교 시 네트워크 지연 차이를 고려해야 함

---

## 핵심 측정 결과

### RTX 3090 성능 측정
- **최적 배치 크기**: 32 (243.3 chunks/second)
- **50페이지 문서 처리**: 2.23초 (354 청크)
- **CPU 대비 처리 시간**: 112초 → 2.23초 (약 50배 단축)
- **OpenAI 대비 처리 시간**: KURE 2.23초 vs OpenAI Large 7.2초

### 품질 측정 결과
- **KURE v1**: 0.4435 평균 유사도
- **OpenAI Large**: 0.3540 평균 유사도
- **OpenAI Small**: 0.3460 평균 유사도

---

## 배치 크기 최적화 측정

| 배치 크기 | 처리 시간 | 처리 속도 | 성능 향상 | GPU 활용도 |
|-----------|-----------|-----------|-----------|------------|
| 1 | 1.71초 | 58.3 chunks/s | 1x | 낮음 |
| 4 | 0.58초 | 172.3 chunks/s | 3x | 중간 |
| 8 | 0.47초 | 214.2 chunks/s | 3.7x | 높음 |
| 16 | 0.45초 | 220.1 chunks/s | 3.8x | 높음 |
| 32 | 0.41초 | 243.3 chunks/s | 4.2x | 최적 |

### 최적화 분석
- 배치 크기 32에서 최고 성능 달성
- 24GB VRAM 메모리 효율적 활용
- CUDA 코어 10,496개 병렬 처리 활용

---

## 문서 크기별 성능 측정

| 문서 크기 | 청크 수 | 처리 시간 | 처리 속도 | 문서 유형 |
|-----------|---------|-----------|-----------|-----------|
| Small (10페이지) | 170 | 0.72초 | 235.2 chunks/s | 보고서, 제안서 |
| Medium (25페이지) | 237 | 1.33초 | 178.0 chunks/s | 기획서, 분석서 |
| Large (50페이지) | 354 | 2.23초 | 158.7 chunks/s | 매뉴얼, 백서 |
| XLarge (100페이지) | 2408 | 4.96초 | 485.4 chunks/s | 기술문서, 책 |

### 성능 특징
- 50페이지 이하 문서: 3초 이내 처리
- 100페이지 문서에서 최고 처리 속도 (485.4 chunks/s)
- 문서 크기에 따른 선형적 성능 확장

---

## OpenAI vs KURE v1 비교 측정

### 50페이지 문서 기준 비교

| 항목 | KURE v1 (RTX 3090) | OpenAI Large | OpenAI Small | 비고 |
|------|---------------------|--------------|--------------|------|
| 처리 시간* | 2.23초 | 7.2초 | 9.3초 | *네트워크 지연 차이 존재 |
| 품질 (유사도) | 0.4435 | 0.3540 | 0.3460 | 한국어 텍스트 기준 |
| 비용 (50페이지) | $0.0000 | $0.0138 | $0.0021 | 로컬 vs API 호출 |
| 월간 비용 (50명) | $50-100 | $2,832.70 | $435.80 | 전력비 vs API 비용 |
| 처리 방식 | 로컬 처리 | 클라우드 | 클라우드 | 데이터 전송 차이 |
| 한국어 특화 | 최적화됨 | 일반적 | 일반적 | 모델 특성 |

*주의: KURE는 로컬 처리 시간, OpenAI는 네트워크 전송 포함 시간

### 상세 측정 분석

#### 처리 시간 측정
```
KURE v1 (로컬):    2.23초
OpenAI Large (API): 7.2초  (네트워크 지연 포함)
OpenAI Small (API): 9.3초  (네트워크 지연 포함)
```

#### 품질 측정 (유사도 점수)
```
KURE v1:      0.4435 (한국어 특화)
OpenAI Large: 0.3540
OpenAI Small: 0.3460
```

#### 비용 구조

**OpenAI 공식 가격 (2024년 기준):**
- text-embedding-3-small: $0.00002 / 1,000 tokens
- text-embedding-3-large: $0.00013 / 1,000 tokens

**50페이지 문서 비용 계산:**
- 총 토큰 수: 354 chunks × 300 tokens/chunk = 106,200 tokens
- Small 모델: 106.2 × $0.00002 = $0.0021
- Large 모델: 106.2 × $0.00013 = $0.0138

**실제 서비스 운영 비용 (50명 사용자 기준):**
```
KURE v1:      로컬 처리 (전력비용만)
OpenAI Small: $435.80/월 (문서 임베딩 + 검색 쿼리 포함)
OpenAI Large: $2,832.70/월 (문서 임베딩 + 검색 쿼리 포함)
```

---

## 처리량 분석

### 시간당 처리량 측정

| 환경 | 시간당 처리 | 일일 처리 | 월간 처리 | 연간 처리 |
|------|-------------|-----------|-----------|-----------|
| KURE v1 (RTX 3090) | 1,614개 | 38,744개 | 1,162,320개 | 13,947,840개 |
| OpenAI Large | 500개 | 12,000개 | 360,000개 | 4,320,000개 |
| OpenAI Small | 387개 | 9,288개 | 278,640개 | 3,343,680개 |
| CPU 처리 | 32개 | 768개 | 23,040개 | 276,480개 |

### 실제 서비스 사용량 분석 (50명 기준)

| 항목 | 월간 사용량 | 토큰 수 | 비용 (Small) | 비용 (Large) |
|------|-------------|---------|--------------|--------------|
| 문서 임베딩 | 200개 문서 | 21,240,000 | $424.80 | $2,761.20 |
| 검색 쿼리 | 11,000회 | 550,000 | $11.00 | $71.50 |
| **총합** | **-** | **21,790,000** | **$435.80** | **$2,832.70** |

### 비용 분석

**토큰 기반 비용 계산 공식:**
```
비용 = (총 토큰 수 ÷ 1,000) × 모델별 단가
- text-embedding-3-small: $0.00002 / 1,000 tokens
- text-embedding-3-large: $0.00013 / 1,000 tokens
```

#### 초기 투자 비용
- RTX 3090: $1,500 (일회성)
- 서버 설정: $500 (일회성)
- 총 투자: $2,000

#### 실제 서비스 운영 비용 (50명 사용자 기준)

**사용 패턴 가정:**
- 월간 신규 문서: 200개 (팀당 4개)
- 사용자당 일일 검색: 10회
- 검색 쿼리 평균 길이: 50 tokens
- 월간 총 검색: 50명 × 10회 × 22일 = 11,000회

**월간 토큰 사용량:**
```
문서 임베딩: 200개 × 106,200 tokens = 21,240,000 tokens
검색 쿼리: 11,000회 × 50 tokens = 550,000 tokens
총 토큰: 21,790,000 tokens
```

**월간 운영 비용 비교:**
- KURE v1: 전력비용만 (약 $50-100)
- OpenAI Large: $2,832.70 (21.79M × $0.00013)
- OpenAI Small: $435.80 (21.79M × $0.00002)

#### 연간 비용 차이 (50명 사용자 기준)
- vs OpenAI Large: $33,992.40 차이 (전력비용 제외)
- vs OpenAI Small: $5,229.60 차이 (전력비용 제외)

**주의사항**: OpenAI 비용은 순수 API 호출 비용이며, 실제로는 네트워크 인프라, 서버 운영비 등 추가 비용이 발생할 수 있습니다.

---

## 사용 시나리오별 성능

### 실시간 검색 서비스
```
10페이지 보고서: 0.72초
25페이지 기획서: 1.33초
50페이지 매뉴얼: 2.23초
```
응답 시간이 3초 이내로 실시간 서비스 구현 가능

### 대량 배치 처리
```
시간당 1,614개 문서 처리
일일 배치: 38,744개 문서
월간 처리: 1,162,320개 문서
```
대규모 문서 아카이브 처리에 적합

### RAG 시스템 구축
```
문서 임베딩: 빠른 처리
벡터 DB 구축: 고속 인덱싱
검색 품질: 한국어 텍스트에서 높은 정확도
```
AI 검색 시스템 구축에 활용 가능

---

## 기술적 특성 분석

### GPU 활용 현황
- VRAM: 24GB 메모리 활용
- CUDA 코어: 10,496개 병렬 처리
- 메모리 대역폭: 936.2 GB/s
- 최적 배치 크기: 32개 동시 처리

### 모델 특성
- 한국어 텍스트 처리에 특화
- Transformer 아키텍처 GPU 최적화
- 로컬 처리로 네트워크 지연 없음
- 일관된 응답 시간 제공

---

## 최적화 권장사항

### 현재 최적 설정
```bash
# GPU 최적화 설정
MAX_BATCH_SIZE=32
OPTIMAL_BATCH_SIZE=32
GPU_MEMORY_FRACTION=0.8
CUDA_VISIBLE_DEVICES=0
```

### 추가 최적화 방향
1. 배치 크기 확장: 64-128로 테스트
2. 멀티 GPU: 여러 RTX 3090 병렬 처리
3. 모델 최적화: TensorRT, ONNX 변환
4. 메모리 최적화: 더 큰 문서 처리

---

## 측정 결과 요약

### 주요 특징
1. 처리 속도: 로컬 환경에서 빠른 처리 (네트워크 지연 없음)
2. 품질: 한국어 텍스트에서 높은 유사도 점수
3. 비용: 로컬 처리로 API 호출 비용 없음
4. 보안: 로컬 처리로 데이터 외부 전송 불필요
5. 확장성: GPU 추가로 처리량 확장 가능

### 적용 가능 분야
- 기업 문서 검색: 빠른 응답 시간
- RAG 시스템: AI 챗봇, 질의응답
- 문서 분석: 대량 문서 자동 분류
- 지식 관리: 기업 지식베이스 구축

---

## 결론

KURE v1과 RTX 3090 조합의 측정 결과:

- 로컬 처리로 빠른 응답 시간 (네트워크 지연 없음)
- 한국어 텍스트에서 높은 품질 점수
- API 호출 비용 없는 로컬 처리
- 데이터 보안을 위한 로컬 처리
- 한국어 특화 모델의 장점

**주의사항**: OpenAI와의 처리 시간 비교 시 네트워크 전송 시간 차이를 고려해야 하며, 원격 서버 환경에서는 성능 차이가 달라질 수 있습니다.

---

*이 리포트는 실제 RTX 3090 GPU에서 측정된 성능 데이터를 바탕으로 작성되었습니다.*
